#!/usr/bin/env python3
"""
Nepali Sentiment Analysis Inference Script
Usage: python predict.py "‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§õ"
"""

import sys
import torch
from transformers import BertForSequenceClassification, AutoTokenizer

def load_model(model_path='./model'):
    """Load the trained model and tokenizer"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = BertForSequenceClassification.from_pretrained(model_path)
        return model, tokenizer
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        sys.exit(1)

def predict_sentiment(text, model, tokenizer):
    """Predict sentiment for given Nepali text"""
    # Prepare input
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )

    # Make prediction
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        prediction = torch.argmax(outputs.logits, dim=1)

    # Map prediction to sentiment
    sentiment_map = {0: "Negative üòû", 1: "Positive üòä", 2: "Neutral üòê"}
    return sentiment_map[prediction.item()]

def main():
    """Main function"""
    # Check command line arguments
    if len(sys.argv) != 2:
        print("Usage: python predict.py \"‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§õ\"")
        sys.exit(1)

    # Get input text
    text = sys.argv[1]

    # Load model
    print("Loading model...")
    model, tokenizer = load_model()

    # Make prediction
    print("\nAnalyzing text...")
    sentiment = predict_sentiment(text, model, tokenizer)

    # Print results
    print("\nResults:")
    print(f"Text: {text}")
    print(f"Sentiment: {sentiment}")

if __name__ == "__main__":
    main()